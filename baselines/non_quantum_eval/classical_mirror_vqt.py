# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UQ7eJY2pVg0k12K34izJoRny0GV3RO-q
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import requests
import random
from collections import Counter


NUM_SENTENCES = 100
MAX_LEN = 9
EMBED_DIM = 16
LEARNING_RATE = 0.001
EPOCHS = 120


url = "https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt"
text_data = requests.get(url).text
raw_sentences = [l.strip() for l in text_data.split('\n') if len(l.split()) > 3]

if len(raw_sentences) > NUM_SENTENCES:
    selected_sentences = random.sample(raw_sentences, NUM_SENTENCES)
else:
    selected_sentences = raw_sentences

vocab = Counter()
for s in selected_sentences:
    vocab.update(s.split())
word2id = {w: i+1 for i, (w, c) in enumerate(vocab.items())}
word2id['<pad>'] = 0
VOCAB_SIZE = len(word2id)

input_ids, target_ids = [], []
for s in selected_sentences:
    ids = [word2id[w] for w in s.split()]
    ids = ids[:MAX_LEN + 1]
    if len(ids) < MAX_LEN + 1:
        ids += [0] * (MAX_LEN + 1 - len(ids))

    input_ids.append(ids[:-1])
    target_ids.append(ids[1:])

x_train = torch.tensor(input_ids)
y_train = torch.tensor(target_ids)

class ClassicalMirrorVQT(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim)

        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=2, batch_first=True)
        self.transformer = nn.TransformerEncoder(layer, num_layers=5)

    def forward(self, x):
        h = self.embedding(x)
        h = F.normalize(h, p=2, dim=-1)

        mask = torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1).to(x.device)

        z_j = self.transformer(h, mask=mask, is_causal=True)

        z_j = F.normalize(z_j, p=2, dim=-1)
        return z_j


def calculate_renyi_loss_mirror(z_j, targets, embedding_layer):
    with torch.no_grad():
        x_target = embedding_layer(targets)
        x_target = F.normalize(x_target, p=2, dim=-1)

    p_j = torch.sum(z_j * x_target, dim=-1)**2
    p_j = torch.clamp(p_j, min=1e-10, max=1.0)

    mask = (targets != 0).float()

    sqrt_p_j = torch.sqrt(p_j)
    mean_sqrt_p = (sqrt_p_j * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1.0)

    loss_per_sentence = -2 * torch.log(mean_sqrt_p + 1e-10)
    return loss_per_sentence.mean()


model = ClassicalMirrorVQT(VOCAB_SIZE, EMBED_DIM)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print(f"Running Mirror Model on {NUM_SENTENCES} random sentences (Max Len: {MAX_LEN})")
print(f"Formula: Loss = -2 * log( mean( sqrt( |<x|z>|^2 ) ) )")

for epoch in range(EPOCHS):
    optimizer.zero_grad()

    z_pred = model(x_train)

    loss = calculate_renyi_loss_mirror(z_pred, y_train, model.embedding)

    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        ppl = np.exp(loss.item())
        print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.6f} | PPL: {ppl:.6f}")