# -*- coding: utf-8 -*-
"""CSAS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gH89AMq6YJCVEP-z4K-HXSEn5MRq_x3h
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import requests
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import KFold

# --- 1. CONFIGURAZIONE ---
EMBED_DIM = 4
D_MODEL = 4
N_HEAD = 1
NUM_TRANSFORMER_LAYERS = 5
LEARNING_RATE = 0.001
EPOCHS = 100
SENTENCE_LENGTH = 5
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import requests
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import KFold
import random  # <--- NUOVO IMPORT

# --- 1. CONFIGURAZIONE ---
EMBED_DIM = 4
D_MODEL = 4
N_HEAD = 1
NUM_TRANSFORMER_LAYERS = 5
LEARNING_RATE = 0.001
EPOCHS = 100
SENTENCE_LENGTH = 5
NUM_SENTENCES = 100  # Dimensione del set di training
TOTAL_NEEDED = NUM_SENTENCES * 2  # 100 Train + 100 Test

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1. Carichiamo TUTTE le frasi valide dal file
raw_sentences = []
try:
    with open('ptb_sentences.txt', 'r', encoding='utf-8') as f:
        for line in f:
            words = line.strip().split()
            # Filtriamo solo le frasi lunghe abbastanza
            if len(words) >= SENTENCE_LENGTH:
                raw_sentences.append(" ".join(words[:SENTENCE_LENGTH]))
except FileNotFoundError:
    print("File non trovato, uso dati dummy.")
    raw_sentences = ["word " * SENTENCE_LENGTH] * TOTAL_NEEDED

# 2. SELEZIONE RANDOMICA
# Controlliamo di averne abbastanza, altrimenti duplichiamo o prendiamo tutto quello che c'è
if len(raw_sentences) >= TOTAL_NEEDED:
    print(f"Trovate {len(raw_sentences)} frasi valide. Ne seleziono {TOTAL_NEEDED} a caso.")
    # random.sample pesca elementi unici a caso senza ripetizioni
    selected_sentences = random.sample(raw_sentences, TOTAL_NEEDED)
else:
    print(f"Attenzione: Trovate solo {len(raw_sentences)} frasi. Le uso tutte (potrebbero essere meno di {TOTAL_NEEDED}).")
    selected_sentences = raw_sentences
    # Se sono meno del necessario per lo split, adattiamo NUM_SENTENCES
    if len(selected_sentences) < TOTAL_NEEDED:
        NUM_SENTENCES = len(selected_sentences) // 2

# 3. Creazione Vocabolario (basato SOLO sulle frasi selezionate per evitare ID vuoti)
vocab = Counter()
for s in selected_sentences: vocab.update(s.split())
word2id = {w: i+1 for i, (w, c) in enumerate(vocab.items())}
word2id['<pad>'] = 0
VOCAB_SIZE = len(word2id)

print(f"Vocabolario costruito: {VOCAB_SIZE} token unici.")

# 4. Funzione per trasformare stringhe in ID
def to_tensor(sentences_list):
    ins, tgts = [], []
    for s in sentences_list:
        ids = [word2id[w] for w in s.split()]
        ins.append(ids[:-1])
        tgts.append(ids[1:])
    return torch.tensor(ins), torch.tensor(tgts)

# 5. DIVISIONE TRAIN / TEST (Randomizzata)
# Primi N per il training
train_subset = selected_sentences[:NUM_SENTENCES]
x_data, y_data = to_tensor(train_subset)

# I successivi N per il test (validazione finale)
test_subset = selected_sentences[NUM_SENTENCES:]
x_val_final, y_val_final = to_tensor(test_subset)

print(f"Training Set shape: {x_data.shape}")
print(f"Test Set shape: {x_val_final.shape}")
# --- 3. FUNZIONI QUANTUM (RÉNYI) ---
def calculate_quantum_metrics(logits: torch.Tensor, targets: torch.Tensor):

    """

    Calcola Loss e PPL seguendo la Perplexity di Rényi (fedeltà media).

    """

    probs = F.softmax(logits, dim=-1)

    p_true = probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)



    # Lunghezza sequenza T

    T = p_true.size(1)



    # 1. Calcoliamo la fedeltà media (F_bar): media delle radici quadrate delle probabilità

    # F_bar = (1/T) * sum(sqrt(p))

    f_bar = torch.mean(torch.sqrt(p_true + 1e-10), dim=1)



    # 2. PPL = F_bar^-2

    ppl_quantum = torch.pow(f_bar, -2)



    # 3. Loss = log(PPL) = -2 * log(f_bar)

    loss_quantum = torch.log(ppl_quantum + 1e-10).mean()



    # PPL media per il log

    true_geometric_ppl = torch.exp(loss_quantum).item()



    return loss_quantum, true_geometric_ppl

# --- 4. ARCHITETTURA MODELLO ---
class SeriousLayer(nn.Module):
    def __init__(self, embed_dim, nhead, dropout=0.1):
        super().__init__()
        # 1. Multi-Head Attention
        self.self_attn = nn.MultiheadAttention(embed_dim, nhead, batch_first=True, dropout=dropout)

        # 2. Feed-Forward Network (FFN)
        # Di solito espande la dimensione per 4 (es. 768 -> 3072 -> 768)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(), # GELU è standard moderno, meglio di ReLU
            nn.Linear(4 * embed_dim, embed_dim),
            nn.Dropout(dropout)
        )

        # 3. Layer Norms (ne servono due)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        # 4. Dropout per le connessioni residuali
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # --- Blocco 1: Attenzione con Residual ---
        # Pre-Norm: Normalizziamo x PRIMA dell'attenzione
        x_norm = self.norm1(x)

        # Calcolo attenzione
        # Nota: in PyTorch nn.MultiheadAttention, attn_mask deve essere gestita attentamente
        attn_output, _ = self.self_attn(query=x_norm, key=x_norm, value=x_norm, attn_mask=mask)

        # Connessione Residuale (Skip Connection): x + Attention(Norm(x))
        x = x + self.dropout(attn_output)

        # --- Blocco 2: Feed-Forward con Residual ---
        x_norm = self.norm2(x)
        ffn_output = self.ffn(x_norm)

        # Connessione Residuale: x + FFN(Norm(x))
        x = x + ffn_output

        return x

class SeriousTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim, nhead, num_layers, context_length=100):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Positional Encoding imparabile (random init è meglio di zeros per iniziare)
        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))

        self.dropout_emb = nn.Dropout(0.1) # Dropout anche sugli embedding

        self.layers = nn.ModuleList([
            SeriousLayer(embed_dim, nhead, dropout=0.1) for _ in range(num_layers)
        ])

        # Normalizzazione finale (necessaria nelle architetture Pre-Norm)
        self.norm_f = nn.LayerNorm(embed_dim)
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        # x shape: (batch_size, seq_len)
        seq_len = x.size(1)

        # Somma embedding + positional encoding
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]
        x = self.dropout_emb(x)

        # Creazione Causal Mask (Triangular mask)
        # 0.0 per visibile, -inf per mascherato
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)

        # Passaggio attraverso i layer
        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm_f(x)
        return self.fc_out(x)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class BarebonesQuantumModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, context_length=100):
        super().__init__()
        # 1. Embedding + Posizione
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))

        # 2. Solo Attenzione (Niente FeedForward, Niente Layer Stack complessi)
        # Usiamo nhead=1 per la massima semplicità
        self.attn = NoSoftmaxAttention(embed_dim, num_heads=1, dropout=0.0)

        # 3. Normalizzazione e Uscita
        self.norm = nn.LayerNorm(embed_dim)
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        seq_len = x.size(1)

        # A. Embedding
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]

        # B. Creazione Maschera Causale (per non guardare il futuro)
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)

        # C. Passaggio nell'Attenzione (con residuale per stabilità)
        attn_out = self.attn(x, mask=mask)
        x = x + attn_out  # Skip connection semplice

        # D. Output
        x = self.norm(x)
        return self.fc_out(x)
# --- 1. MODULO ATTENZIONE CUSTOM (SENZA SOFTMAX) ---
class NoSoftmaxAttention(nn.Module):
    """
    Implementazione manuale di MultiHeadAttention ma SENZA la Softmax sui coefficienti.
    Mantiene esattamente lo stesso numero di parametri di nn.MultiheadAttention.
    """
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Scaling factor 1/sqrt(d_k)
        self.scale = self.head_dim ** -0.5

        assert self.head_dim * num_heads == embed_dim, "embed_dim deve essere divisibile per num_heads"

        # Le 4 proiezioni lineari standard (Q, K, V, Output)
        # Queste hanno esattamente lo stesso peso di quelle dentro nn.MultiheadAttention
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # x shape: (Batch, Seq_Len, Embed_Dim)
        batch_size, seq_len, _ = x.shape

        # 1. Proiezioni Lineari
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # 2. Divisione in Heads (anche se è 1, manteniamo la struttura per compatibilità)
        # Trasformiamo in (Batch, Head, Seq, Head_Dim)
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # 3. Calcolo Attenzione "Raw" (Linear Interference)
        # Moltiplicazione Q * K^T -> Shape: (Batch, Head, Seq, Seq)
        attn_scores = (q @ k.transpose(-2, -1)) * self.scale

        # 4. Gestione Maschera (CRUCIALE)
        # In standard attention: mask=-inf diventa softmax(-inf)=0.
        # Qui SENZA softmax: dobbiamo forzare a 0.0 esplicitamente, altrimenti -inf spacca tutto.
        if mask is not None:
            # Se la maschera usa -inf per nascondere, lo convertiamo a 0
            # Se la maschera usa True/False (True=Masked), usiamo masked_fill
            # Assumiamo maschera additiva standard (0 o -inf)
            attn_scores = attn_scores.masked_fill(mask == float('-inf'), 0.0)

        # NIENTE SOFTMAX QUI!
        # I valori possono essere negativi (interferenza distruttiva)

        # Applichiamo dropout sui coefficienti
        attn_probs = self.dropout(attn_scores)

        # 5. Moltiplicazione per V e ricombinazione
        out = attn_probs @ v

        # Torna alla forma originale (Batch, Seq, Embed_Dim)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)

        # Proiezione finale
        return self.out_proj(out)

# --- 2. LAYER DEL TRASFORMER MODIFICATO ---
class SeriousLayerNoSoftmax(nn.Module):
    def __init__(self, embed_dim, nhead, dropout=0.1):
        super().__init__()

        # Usiamo l'attenzione custom definita sopra
        self.self_attn = NoSoftmaxAttention(embed_dim, nhead, dropout=dropout)

        # FFN Standard (Expansion factor 4)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(),
            nn.Linear(4 * embed_dim, embed_dim),
            nn.Dropout(dropout)
        )

        # Normalizzazioni
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # Pre-Norm Architecture
        x_norm = self.norm1(x)

        # Attenzione (senza softmax)
        attn_output = self.self_attn(x_norm, mask=mask)
        x = x + self.dropout(attn_output) # Residual

        # FFN
        x_norm = self.norm2(x)
        ffn_output = self.ffn(x_norm)
        x = x + ffn_output # Residual

        return x

# --- 3. IL MODELLO FINALE DA CHIAMARE ---
class SeriousTransformerNoSoftmax(nn.Module):
    def __init__(self, vocab_size, embed_dim, nhead, num_layers, context_length=100):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # Positional Encoding
        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))
        self.dropout_emb = nn.Dropout(0.1)

        # Stack di layer senza softmax
        self.layers = nn.ModuleList([
            SeriousLayerNoSoftmax(embed_dim, nhead, dropout=0.1) for _ in range(num_layers)
        ])

        self.norm_f = nn.LayerNorm(embed_dim)
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        seq_len = x.size(1)
        # Embedding + Positional
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]
        x = self.dropout_emb(x)

        # Causal Mask
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)

        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm_f(x)
        return self.fc_out(x)

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --- 1. MODULO ATTENZIONE CUSTOM (SENZA SOFTMAX) ---
class NoSoftmaxAttention(nn.Module):
    """
    Implementazione manuale di MultiHeadAttention ma SENZA la Softmax sui coefficienti.
    Mantiene esattamente lo stesso numero di parametri di nn.MultiheadAttention.
    """
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Scaling factor 1/sqrt(d_k)
        self.scale = self.head_dim ** -0.5

        assert self.head_dim * num_heads == embed_dim, "embed_dim deve essere divisibile per num_heads"

        # Le 4 proiezioni lineari standard (Q, K, V, Output)
        # Queste hanno esattamente lo stesso peso di quelle dentro nn.MultiheadAttention
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # x shape: (Batch, Seq_Len, Embed_Dim)
        batch_size, seq_len, _ = x.shape

        # 1. Proiezioni Lineari
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # 2. Divisione in Heads (anche se è 1, manteniamo la struttura per compatibilità)
        # Trasformiamo in (Batch, Head, Seq, Head_Dim)
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # 3. Calcolo Attenzione "Raw" (Linear Interference)
        # Moltiplicazione Q * K^T -> Shape: (Batch, Head, Seq, Seq)
        attn_scores = (q @ k.transpose(-2, -1)) * self.scale

        # 4. Gestione Maschera (CRUCIALE)
        # In standard attention: mask=-inf diventa softmax(-inf)=0.
        # Qui SENZA softmax: dobbiamo forzare a 0.0 esplicitamente, altrimenti -inf spacca tutto.
        if mask is not None:
            # Se la maschera usa -inf per nascondere, lo convertiamo a 0
            # Se la maschera usa True/False (True=Masked), usiamo masked_fill
            # Assumiamo maschera additiva standard (0 o -inf)
            attn_scores = attn_scores.masked_fill(mask == float('-inf'), 0.0)

        # NIENTE SOFTMAX QUI!
        # I valori possono essere negativi (interferenza distruttiva)

        # Applichiamo dropout sui coefficienti
        attn_probs = self.dropout(attn_scores)

        # 5. Moltiplicazione per V e ricombinazione
        out = attn_probs @ v

        # Torna alla forma originale (Batch, Seq, Embed_Dim)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)

        # Proiezione finale
        return self.out_proj(out)

# --- 2. LAYER DEL TRASFORMER MODIFICATO ---
class SeriousLayerNoSoftmax(nn.Module):
    def __init__(self, embed_dim, nhead, dropout=0.1):
        super().__init__()

        # Usiamo l'attenzione custom definita sopra
        self.self_attn = NoSoftmaxAttention(embed_dim, nhead, dropout=dropout)

        # FFN Standard (Expansion factor 4)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(),
            nn.Linear(4 * embed_dim, embed_dim),
            nn.Dropout(dropout)
        )

        # Normalizzazioni
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # Pre-Norm Architecture
        x_norm = self.norm1(x)

        # Attenzione (senza softmax)
        attn_output = self.self_attn(x_norm, mask=mask)
        x = x + self.dropout(attn_output) # Residual

        # FFN
        x_norm = self.norm2(x)
        ffn_output = self.ffn(x_norm)
        x = x + ffn_output # Residual

        return x

# --- 3. IL MODELLO FINALE DA CHIAMARE ---
class SeriousTransformerNoSoftmax(nn.Module):
    def __init__(self, vocab_size, embed_dim, nhead, num_layers, context_length=100):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # Positional Encoding
        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))
        self.dropout_emb = nn.Dropout(0.1)

        # Stack di layer senza softmax
        self.layers = nn.ModuleList([
            SeriousLayerNoSoftmax(embed_dim, nhead, dropout=0.1) for _ in range(num_layers)
        ])

        self.norm_f = nn.LayerNorm(embed_dim)
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        seq_len = x.size(1)
        # Embedding + Positional
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]
        x = self.dropout_emb(x)

        # Causal Mask
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)

        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm_f(x)
        return self.fc_out(x)

import torch.nn as nn
import torch.nn.functional as F

class SeriousTransformerIsometric(nn.Module):
    def __init__(self, vocab_size, embed_dim, nhead, num_layers, context_length=100):
        super().__init__()

        # E: Matrice di Embedding (V x D)
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # V: Matrice di Rotazione Allenabile (D x D)
        # In encoding.py la chiami rotationMatrix
        self.rotation = nn.Parameter(torch.empty(embed_dim, embed_dim))

        # Inizializzazione ortogonale (come nel tuo _buildEmbeddingMatrix e _buildRotationMatrix)
        nn.init.orthogonal_(self.embedding.weight)
        nn.init.orthogonal_(self.rotation)

        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))
        self.dropout_emb = nn.Dropout(0.1)

        # Layer senza softmax (quelli definiti precedentemente)
        self.layers = nn.ModuleList([
            SeriousLayerNoSoftmax(embed_dim, nhead, dropout=0.1) for _ in range(num_layers)
        ])

        self.norm_f = nn.LayerNorm(embed_dim)
        self.fc_out_bias = nn.Parameter(torch.zeros(vocab_size))

    def forward(self, x):
        seq_len = x.size(1)

        # --- FASE E (Embedding) ---
        x = self.embedding(x)
        x = x + self.pos_encoder[:, :seq_len, :]
        x = self.dropout_emb(x)

        for layer in self.layers:
            x = layer(x, mask=torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device))

        x = self.norm_f(x)

        # --- FASE F (De-embedding isometrico e nello stesso range) ---
        # Calcoliamo F "al volo": F = E @ V (come in encoding.py)
        # E è (V x D), V è (D x D) -> F è (V x D)
        F_eff = self.embedding.weight @ self.rotation

        # Logits: x @ F_eff.T
        logits = F.linear(x, F_eff, bias=self.fc_out_bias)

        return logits

def orthogonal_penalty(matrix):
    # Forza M^T * M ≈ I
    dim = matrix.size(-1)
    identity = torch.eye(dim, device=matrix.device)
    res = torch.mm(matrix.t(), matrix)
    return torch.norm(res - identity)

# Nel loop di training:

model = SeriousTransformer(
    vocab_size=VOCAB_SIZE,
    embed_dim=EMBED_DIM,
    nhead=N_HEAD,
    num_layers=NUM_TRANSFORMER_LAYERS,
    context_length=SENTENCE_LENGTH
).to(device)

modelWITHOUTCSA = SeriousTransformerNoSoftmax(
    vocab_size=VOCAB_SIZE,
    embed_dim=EMBED_DIM,
    nhead=N_HEAD,
    num_layers=NUM_TRANSFORMER_LAYERS,
    context_length=SENTENCE_LENGTH
).to(device)

modeEMPTY = BarebonesQuantumModel(
    vocab_size=VOCAB_SIZE,
    embed_dim=EMBED_DIM,
    context_length=SENTENCE_LENGTH
).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

loss_history = []
ppl_history = []

print(f"Inizio training Quantum su {len(x_data)} frasi...")
for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()

    logits = model(x_data)
    loss, ppl_val = calculate_quantum_metrics(logits, y_data)

    loss.backward()
    optimizer.step()

    loss_history.append(loss.item())
    ppl_history.append(ppl_val)

    print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | PPL : {ppl_val:.4f}")
print("\n--- VALUTAZIONE 3-FOLD (PPL & ERRORE) ---")
kf = KFold(n_splits=3)
fold_ppl_results = []
fold_err_results = []

model.eval()
with torch.no_grad():
    for fold_idx, (_, test_idx) in enumerate(kf.split(x_data)):
        x_test, y_test = x_val_final[test_idx].to(device), y_val_final[test_idx].to(device)
        logits_fold = model(x_test)

        # 1. Metrica PPL (Quantum Logic)
        _, ppl_fold = calculate_quantum_metrics(logits_fold, y_test)

        # 2. Metrica ERRORE (Classica Hard-Argmax)
        # Chi ha il valore più alto?
        pred_fold = torch.argmax(logits_fold, dim=-1)

        # Quanti ne ho indovinati?
        correct = (pred_fold == y_test).float().sum()
        total = y_test.numel()

        # Errore = 1 - Accuracy
        accuracy = correct / total
        err_fold = 1.0 - accuracy.item()

        fold_ppl_results.append(ppl_fold)
        fold_err_results.append(err_fold)

        print(f"[Fold {fold_idx+1}] PPL: {ppl_fold:.4f} | Error Rate: {err_fold:.4f} ({err_fold*100:.2f}%)")

# --- 7. PLOT (Solo Training) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Quantum Loss')
plt.title('Training Loss (Rényi)'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(ppl_history, color='orange', label='Quantum PPL')
plt.title('Training Perplexity'); plt.legend()
plt.show()

# --- 8. RISULTATI FINALI ---
print(f"\n[FINALE] PPL Media :   {np.mean(fold_ppl_results):.4f}")
print(f"[FINALE] Error Medio : {np.mean(fold_err_results):.4f}")


model = modelWITHOUTCSA
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

loss_history = []
ppl_history = []

print(f"Inizio training Quantum su {len(x_data)} frasi... NO SMAX")
for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()

    logits = model(x_data)
    loss, ppl_val = calculate_quantum_metrics(logits, y_data)

    loss.backward()
    optimizer.step()

    loss_history.append(loss.item())
    ppl_history.append(ppl_val)

    print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | PPL : {ppl_val:.4f}")
print("\n--- VALUTAZIONE 3-FOLD (PPL & ERRORE) NO SMAX ---")
kf = KFold(n_splits=3)
fold_ppl_results = []
fold_err_results = []

model.eval()
with torch.no_grad():
    for fold_idx, (_, test_idx) in enumerate(kf.split(x_data)):
        x_test, y_test = x_val_final[test_idx].to(device), y_val_final[test_idx].to(device)
        logits_fold = model(x_test)

        # 1. Metrica PPL (Quantum Logic)
        _, ppl_fold = calculate_quantum_metrics(logits_fold, y_test)

        # 2. Metrica ERRORE (Classica Hard-Argmax)
        # Chi ha il valore più alto?
        pred_fold = torch.argmax(logits_fold, dim=-1)

        # Quanti ne ho indovinati?
        correct = (pred_fold == y_test).float().sum()
        total = y_test.numel()

        # Errore = 1 - Accuracy
        accuracy = correct / total
        err_fold = 1.0 - accuracy.item()

        fold_ppl_results.append(ppl_fold)
        fold_err_results.append(err_fold)

        print(f"[Fold {fold_idx+1}] PPL: {ppl_fold:.4f} | Error Rate: {err_fold:.4f} ({err_fold*100:.2f}%)")

# --- 7. PLOT (Solo Training) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Quantum Loss')
plt.title('Training Loss (Rényi)'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(ppl_history, color='orange', label='Quantum PPL')
plt.title('Training Perplexity'); plt.legend()
plt.show()

# --- 8. RISULTATI FINALI ---
print(f"\n[FINALE] PPL Media NO SMAX:   {np.mean(fold_ppl_results):.4f}")
print(f"[FINALE] Error Medio NO SMAX: {np.mean(fold_err_results):.4f}")


model = modeEMPTY
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

loss_history = []
ppl_history = []

print(f"Inizio training Quantum su {len(x_data)} frasi... NO SMAX")
for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()

    logits = model(x_data)
    loss, ppl_val = calculate_quantum_metrics(logits, y_data)

    loss.backward()
    optimizer.step()

    loss_history.append(loss.item())
    ppl_history.append(ppl_val)

    print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | PPL : {ppl_val:.4f}")
print("\n--- VALUTAZIONE 3-FOLD (PPL & ERRORE) NO SMAX ---")
kf = KFold(n_splits=3)
fold_ppl_results = []
fold_err_results = []

model.eval()
with torch.no_grad():
    for fold_idx, (_, test_idx) in enumerate(kf.split(x_data)):
        x_test, y_test = x_val_final[test_idx].to(device), y_val_final[test_idx].to(device)
        logits_fold = model(x_test)

        # 1. Metrica PPL (Quantum Logic)
        _, ppl_fold = calculate_quantum_metrics(logits_fold, y_test)

        # 2. Metrica ERRORE (Classica Hard-Argmax)
        # Chi ha il valore più alto?
        pred_fold = torch.argmax(logits_fold, dim=-1)

        # Quanti ne ho indovinati?
        correct = (pred_fold == y_test).float().sum()
        total = y_test.numel()

        # Errore = 1 - Accuracy
        accuracy = correct / total
        err_fold = 1.0 - accuracy.item()

        fold_ppl_results.append(ppl_fold)
        fold_err_results.append(err_fold)

        print(f"[Fold {fold_idx+1}] PPL: {ppl_fold:.4f} | Error Rate: {err_fold:.4f} ({err_fold*100:.2f}%)")

# --- 7. PLOT (Solo Training) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Quantum Loss')
plt.title('Training Loss (Rényi)'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(ppl_history, color='orange', label='Quantum PPL')
plt.title('Training Perplexity'); plt.legend()
plt.show()

# --- 8. RISULTATI FINALI ---
print(f"\n[FINALE] PPL Media NO SMAX:   {np.mean(fold_ppl_results):.4f}")
print(f"[FINALE] Error Medio NO SMAX: {np.mean(fold_err_results):.4f}")

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

# --- FUNZIONE DI REGOLARIZZAZIONE ORTOGONALE ---
def orthogonal_penalty(matrix):
    """Forza la matrice M affinché M^T * M ≈ I (Isometria)."""
    dim = matrix.size(-1)
    identity = torch.eye(dim, device=matrix.device)
    # Calcola il prodotto tra la trasposta e la matrice stessa
    res = torch.mm(matrix.t(), matrix)
    return torch.norm(res - identity)

# --- 1. DEFINIZIONE MODELLO ISOMETRICO ---
class SeriousTransformerIsometric(nn.Module):
    def __init__(self, vocab_size, embed_dim, nhead, num_layers, context_length=100):
        super().__init__()
        # E: Matrice di Embedding (V x D)
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # V: Matrice di Rotazione (D x D) - Definita in encoding.py
        self.rotation = nn.Parameter(torch.empty(embed_dim, embed_dim))

        # Inizializzazione Ortogonale (Isometria iniziale)
        nn.init.orthogonal_(self.embedding.weight)
        nn.init.orthogonal_(self.rotation)

        self.pos_encoder = nn.Parameter(torch.randn(1, context_length, embed_dim))
        self.dropout_emb = nn.Dropout(0.1)

        # Layer senza Softmax
        self.layers = nn.ModuleList([
            SeriousLayerNoSoftmax(embed_dim, nhead, dropout=0.1) for _ in range(num_layers)
        ])

        self.norm_f = nn.LayerNorm(embed_dim)
        self.fc_out_bias = nn.Parameter(torch.zeros(vocab_size))

    def forward(self, x):
        seq_len = x.size(1)
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]
        x = self.dropout_emb(x)

        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)

        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm_f(x)

        # Calcolo F (De-embedding) come in encoding.py: F = E @ V
        # Questo garantisce stesso range e isometria se E e V sono ortogonali.
        F_eff = self.embedding.weight @ self.rotation

        # Logits calcolati usando la matrice F risultante
        return F.linear(x, F_eff, bias=self.fc_out_bias)

# --- 2. INIZIALIZZAZIONE ---
model = SeriousTransformerIsometric(
    vocab_size=VOCAB_SIZE,
    embed_dim=EMBED_DIM,
    nhead=N_HEAD,
    num_layers=NUM_TRANSFORMER_LAYERS,
    context_length=SENTENCE_LENGTH
).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

loss_history = []
ppl_history = []

# --- 3. LOOP DI TRAINING CON VINCOLI ISOMETRICI ---
print(f"Inizio training Quantum Isometrico su {len(x_data)} frasi...")

for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()

    logits = model(x_data)

    # 1. Quantum Loss (Rényi)
    loss_q, ppl_val = calculate_quantum_metrics(logits, y_data)

    # 2. Vincoli strutturali (da encoding.py)
    # Forza E^T E = I
    reg_E = orthogonal_penalty(model.embedding.weight)
    # Forza V^T V = I (Garantisce F^T F = I e F F^T = E E^T)
    reg_V = orthogonal_penalty(model.rotation)

    # Loss Totale: Quantum + Penalità Isometria
    total_loss = loss_q + 0.1 * (reg_E + reg_V)

    total_loss.backward()
    optimizer.step()

    loss_history.append(total_loss.item())
    ppl_history.append(ppl_val)

    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(f"Epoch {epoch+1:03d} | Total Loss: {total_loss.item():.4f} | PPL: {ppl_val:.4f}")

# --- 4. VALUTAZIONE 3-FOLD ---
print("\n--- VALUTAZIONE 3-FOLD (PPL & ERRORE) ---")
kf = KFold(n_splits=3)
fold_ppl_results, fold_err_results = [], []

model.eval()
with torch.no_grad():
    E = model.embedding.weight
    V = model.rotation
    F_mat = E @ V

    e_iso = torch.allclose(E.t() @ E, torch.eye(EMBED_DIM).to(device), atol=1e-2)
    v_iso = torch.allclose(V.t() @ V, torch.eye(EMBED_DIM).to(device), atol=1e-2)

    # Verifica il Check 3 del tuo file: F F^T == E E^T
    range_check = torch.allclose(F_mat @ F_mat.t(), E @ E.t(), atol=1e-2)

    print(f"Check Isometria E: {e_iso}")
    print(f"Check Isometria V: {v_iso}")
    print(f"Check Stesso Range (F F^T == E E^T): {range_check}")
    for fold_idx, (_, test_idx) in enumerate(kf.split(x_data)):
        x_test, y_test = x_val_final[test_idx].to(device), y_val_final[test_idx].to(device)
        logits_fold = model(x_test)

        _, ppl_fold = calculate_quantum_metrics(logits_fold, y_test)

        pred_fold = torch.argmax(logits_fold, dim=-1)
        correct = (pred_fold == y_test).float().sum()
        err_fold = 1.0 - (correct / y_test.numel()).item()

        fold_ppl_results.append(ppl_fold)
        fold_err_results.append(err_fold)

        print(f"[Fold {fold_idx+1}] PPL: {ppl_fold:.4f} | Error Rate: {err_fold*100:.2f}%")

# --- 5. PLOT E RISULTATI ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1); plt.plot(loss_history, label='Total Loss'); plt.title('Training Loss'); plt.legend()
plt.subplot(1, 2, 2); plt.plot(ppl_history, color='orange', label='Quantum PPL'); plt.title('Training Perplexity'); plt.legend()
plt.show()

print(f"\n[FINALE] PPL Media : {np.mean(fold_ppl_results):.4f}")
print(f"[FINALE] Error Medio : {np.mean(fold_err_results):.4f}")